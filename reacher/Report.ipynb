{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2 Report - Continous Control\n",
    "\n",
    "- Author: Thiago Akio Nakamura\n",
    "- Date: May 2021\n",
    "- Repo: https://github.com/akionakamura/drlnd-p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This document contains the report for the Project 2 of the Deep Reinforcement Learning Nano Degree from Udacity. The goal of this project is to train an reinforcement learning agent to for a continous control problem. This report explains the algorithms used, as well as the tests parameters and the obtained results. The reader is also encouraged to read through the source code, for the detailed implementations of the neural networks and the agent itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment\n",
    "The `Reach` environment consists in a dual-jointed robotic arm that needs to follow a target location. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible. Its observation space has 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. The action space corresponds to 4 continuous variables varying between [-1, 1], where each corresponds to torque applicable to two joints.\n",
    "\n",
    "#### The Goal\n",
    "The task is episodic, and in order to solve the environment, the agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akio/.pyenv/versions/3.6.12/envs/drlnd-p2/lib/python3.6/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import asdict\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from agents import MultiAgent\n",
    "from experiment import RunExperiments, RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants for the project.\n",
    "# Number of episodes to train all agents.\n",
    "NUM_EPISODES = 500\n",
    "\n",
    "# Consecutive runs to average the scores.\n",
    "RESULT_WINDOW = 100\n",
    "\n",
    "# Minimum average score over the window to complete the project.\n",
    "MIN_AVERAGE_SCORE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_to_run = RunExperiments(\n",
    "    learn_steps=[1, 2, 4],\n",
    "    sync_steps=[4, 8, 16],\n",
    "    batch_sizes=[16, 32, 64],\n",
    "    gammas=[0.95, 0.99],\n",
    "    epsilon_decays=[0.98, 0.99]\n",
    ")\n",
    "NUM_EXPERIMENTS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithms\n",
    "\n",
    "We are trying to solve the problem with Q-Learning, and besides the basic approach, we are also going to try the DoubleDQN [1] and Dueling DQN [2] approaches.\n",
    "\n",
    "#### DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_episodes, env, agent):\n",
    "    all_scores = []\n",
    "    score_window = deque(maxlen=RESULT_WINDOW)\n",
    "    solved = False\n",
    "    best_mean_score = 0\n",
    "    \n",
    "    for episode_i in range(num_episodes):\n",
    "    \n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            \n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        \n",
    "        agent.episode_finished()\n",
    "        avg_score = np.mean(scores)\n",
    "        score_window.append(avg_score)\n",
    "        all_scores.append(avg_score)\n",
    "        \n",
    "        current_mean = np.mean(score_window)\n",
    "        if current_mean >= MIN_AVERAGE_SCORE:\n",
    "            solved = True\n",
    "        \n",
    "        if current_mean > best_mean_score:\n",
    "            best_mean_score = current_mean\n",
    "        print(f\"\\r  Average score on episode {episode_i}: {current_mean}\", end=\"\")\n",
    "        \n",
    "    return all_scores, solved, best_mean_score, current_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "#### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment 0 with config: RunConfig(learn_step=1, sync_step=8, batch_size=64, gamma=0.95, epsilon_decay=0.99)\n",
      "  Average score on episode 9: 0.362449991898611276"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment_scores = {}\n",
    "mlflow.set_experiment(\"reacher-ddpg\")\n",
    "for i, config in enumerate(experiments_to_run.get_configs(NUM_EXPERIMENTS)):\n",
    "    with mlflow.start_run(run_name=f\"Config {i}\"):\n",
    "        start_time = time.time()\n",
    "        config = experiments_to_run.get_random()\n",
    "        print(f\"Running experiment {i} with config: {config}\")\n",
    "        mlflow.log_params(asdict(config))\n",
    "        \n",
    "        agent = MultiAgent(\n",
    "            num_agents,\n",
    "            state_size,\n",
    "            action_size,\n",
    "            gamma=config.gamma,\n",
    "            learn_step=config.learn_step,\n",
    "            sync_step=config.sync_step,\n",
    "            epsilon_decay=config.epsilon_decay,\n",
    "            batch_size=config.batch_size\n",
    "        )\n",
    "\n",
    "        scores, solved, best_mean_score, last_mean_score = train(NUM_EPISODES, env, agent)\n",
    "        experiment_scores[config] = scores\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"  Ran experiment in {elapsed} seconds.\")\n",
    "        print(f\"  Best mean score of {best_mean_score}.\")\n",
    "        print(f\"  Latest mean score of {last_mean_score}.\")\n",
    "        mlflow.log_metric(\"best_mean_score\", best_mean_score)\n",
    "        mlflow.log_metric(\"last_mean_score\", last_mean_score)\n",
    "\n",
    "        if solved:\n",
    "            print(\"  Solved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "for c, scores in experiment_scores.items():\n",
    "    plt.plot(scores, label=c, alpha=0.7)\n",
    "    \n",
    "plt.title(\"Experiment Results\")\n",
    "plt.ylabel(\"Average Score\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.legend(bbox_to_anchor=(0.5, -0.08))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
